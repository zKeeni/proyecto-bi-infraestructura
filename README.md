# ğŸŒ± Proyecto BI â€“ Infraestructura de AnÃ¡lisis del Cacao Ecuatoriano

Este repositorio contiene la configuraciÃ³n de infraestructura para un proyecto de **Business Intelligence (BI)** orientado al anÃ¡lisis de datos de exportaciÃ³n de **cacao ecuatoriano**.  
El entorno se basa en **contenedores Docker**, integrando herramientas de orquestaciÃ³n, almacenamiento y visualizaciÃ³n de datos.

---

## ğŸ’» Entorno de EjecuciÃ³n

Este proyecto fue desarrollado y probado en un entorno **Linux (Ubuntu 24.04)**.  
Si te encuentras en **Windows**, puedes usar **WSL (Windows Subsystem for Linux)** para ejecutar el proyecto.

### ğŸ§© InstalaciÃ³n de WSL en Windows
1. Abre **PowerShell como administrador** y ejecuta:
   ```bash
   wsl --install
   ```
2. Reinicia tu computadora cuando el sistema lo solicite.
3. Al volver a iniciar, selecciona Ubuntu como distribuciÃ³n predeterminada (o instÃ¡lala desde Microsoft Store).
4. Luego abre tu terminal de Ubuntu desde Windows y continÃºa con las instrucciones de este README desde allÃ­.

âš™ï¸ Requisitos del Sistema

Para ejecutar correctamente todo el entorno (Airflow + Hive + Redash), se recomienda contar con un equipo con recursos suficientes para soportar la carga de los tres contenedores.

Requisitos mÃ­nimos:

ğŸ’» Procesador: Intel Core i7 (10.Âª generaciÃ³n o superior)

ğŸ§  Memoria RAM: 16 GB

ğŸ§ Sistema operativo: Linux nativo o WSL2 con Ubuntu 22.04 o superior

ğŸ’¾ Almacenamiento: El espacio necesario dependerÃ¡ del tamaÃ±o de los datos y de los contenedores; WSL o Linux gestionan esto internamente.

Equipo en el que se probÃ³ el proyecto:

ğŸ’» Procesador: Intel Core i9 (13.Âª generaciÃ³n)

ğŸ§  Memoria RAM: 32 GB

>âš ï¸ RecomendaciÃ³n:
Durante las pruebas, el entorno completo llegÃ³ a utilizar mÃ¡s de 16 GB de RAM.
Si tu equipo cuenta con 16 GB, es posible que notes lentitud o sobrecarga al ejecutar los tres contenedores simultÃ¡neamente.
En ese caso, se sugiere disponer de mÃ¡s memoria o levantar los servicios por separado.

El espacio en disco lo gestiona internamente WSL o el sistema Linux.

## ğŸ§° TecnologÃ­as Utilizadas

- **Apache Airflow** â€“ OrquestaciÃ³n de flujos ETL  
- **Apache Hive** â€“ Almacenamiento y procesamiento de datos  
- **Redash** â€“ VisualizaciÃ³n y dashboards interactivos  
- **Docker Compose** â€“ GestiÃ³n de contenedores y redes  

---

## âš™ï¸ Requisitos Previos

Antes de comenzar, asegÃºrate de tener instalado **Docker** y **Docker Compose** en tu sistema.

ğŸ“˜ [Instalar Docker](https://docs.docker.com/get-docker/)  
ğŸ“˜ [Instalar Docker Compose](https://docs.docker.com/compose/install/)

---

##ğŸ§© InstalaciÃ³n y ConfiguraciÃ³n

### 1ï¸âƒ£ Clonar el repositorio
```bash
git clone https://github.com/zKeeni/proyecto-bi-infraestructura.git
```
Acceda al directorio de la carpeta clonada
```bash
cd proyecto-bi-infraestructura/
```
### 2ï¸âƒ£ Configurar Apache Airflow
```bash
cd airflow
```

Crear directorios necesarios:
```bash
mkdir -p logs plugins
```
(Estos directorios son ignorados por Git y deben crearse localmente para que Airflow funcione correctamente.)

Levantar los contenedores:
```bash
docker compose up -d
```

Airflow quedarÃ¡ disponible en:
ğŸ‘‰ http://localhost:8080

### 3ï¸âƒ£ Configurar Apache Hive
```bash
cd ../hive
```

Crear los directorios de datos:
```bash
mkdir -p hdfs hive-logs hive_warehouse postgres_data
```

Asignar permisos (si es necesario):
```bash
sudo chmod -R 777 hdfs hive-logs hive_warehouse postgres_data
```

Levantar los contenedores:
```bash
cd compose/
```
```bash
docker compose up -d
```

Hive Server2 estarÃ¡ disponible en el puerto configurado (por defecto: 10000).

### 4ï¸âƒ£ Configurar Redash
```bash
cd ../../redash
```

Crear el archivo .env:
Ejemplo de configuraciÃ³n funcional (ya incluida en este repositorio):

```env
# Claves internas de Redash
REDASH_SECRET_KEY=supersecretkey
REDASH_COOKIE_SECRET=anothersecretkey

# Base de datos de Redash
POSTGRES_PASSWORD=postgres
REDASH_DATABASE_URL=postgresql://postgres:postgres@redash_postgres/postgres

# Redis (tareas en segundo plano)
REDASH_REDIS_URL=redis://redis:6379/0
```

Si desea ocupar esta misma configuraciÃ³n ya incluida, entonces cambie el nombre del archivo de ejemplo a .env
```bash
mv .env.example .env
```


ğŸ’¡ Puedes cambiar estas claves si deseas mayor seguridad, pero recuerda actualizar el docker-compose.yml en consecuencia.

Levantar Redash:
```bash
docker compose up -d
```

Una vez iniciado, Redash estarÃ¡ disponible en:
ğŸ‘‰ http://localhost:5000
--- 
## ğŸ§  Uso del Proyecto

Una vez que los tres servicios estÃ©n activos (Airflow, Hive y Redash), sigue los pasos a continuaciÃ³n para ejecutar los procesos ETL y visualizar los dashboards.

### ğŸª¶ En Airflow

1. Abre Airflow:
ğŸ‘‰ http://localhost:8080/dags

2. En la barra de bÃºsqueda, escribe cargar para listar los DAGs del proyecto.
DeberÃ¡s ver los DAGs similares a la siguiente imagen
<img width="1919" height="1137" alt="image" src="https://github.com/user-attachments/assets/9bc61b0d-4f5b-46ba-8e7c-c12cc88ebdc9" />

Ejecuta los DAGs en el siguiente orden:

ğŸ—ƒï¸ Tras ejecutar estos DAGs, la base de datos cacao se escribirÃ¡ dentro de Hive.
AsegÃºrate de tener ambos contenedores (Airflow y Hive) levantados

### ğŸ“Š En Redash

1. Abre Redash:
ğŸ‘‰ http://localhost:5000

2. Ingresa las siguientes credenciales:
Usuario: admin@email.com
ContraseÃ±a: admin123

Una vez dentro del panel de control, accede a Dashboards.

PodrÃ¡s refrescar los dashboards con la informaciÃ³n cargada desde Hive.

ğŸ”— En Redash, toda la configuraciÃ³n de conexiÃ³n, consultas SQL y dashboards ya estÃ¡ preconfigurada.
Solo asegÃºrate de que los pasos de Airflow y Hive se hayan ejecutado correctamente para evitar errores de datos inexistentes.
